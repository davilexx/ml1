
# Метод опорных векторов (SVM)

<b>Метод SVM (support vector machine)</b> обладает несколькими свойствами:
<br/>
• обучение SVM сводится к задаче квадратичного программирования, имеющей единственное решение, которое вычисляется достаточно эффективно даже на выборках
в сотни тысяч объектов;
<br/>
• решение обладает свойством разреженности: положение оптимальной разделяющей гиперплоскости зависит лишь от небольшой
доли обучающих объектов, которые и называются опорными векторами, остальные объекты фактически не задействуются;
<br/>
• с помощью введения функции ядра метод обобщается на случай нелинейных разделяющих поверхностей.
<br/>

<h2>Линейно разделимая выборка</h2>

Рассмотрим задачу классификации на два непересекающихся класса, в которой объекты описываются n-мерными вещественными векторами:
<br/>
<img src="https://github.com/davilexx/ml1/blob/master/%D0%9B%D0%B8%D0%BD%D0%B5%D0%B8%CC%86%D0%BD%D1%8B%D0%B5%20%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%D1%8B%20%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D0%B8/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4%20%D0%BE%D0%BF%D0%BE%D1%80%D0%BD%D1%8B%D1%85%20%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BE%D0%B2/img/Screenshot%202020-11-17%20at%2015.13.47.png" width="20%">

Будем строить линейный пороговый классификатор:
<br/>
<img src="https://github.com/davilexx/ml1/blob/master/%D0%9B%D0%B8%D0%BD%D0%B5%D0%B8%CC%86%D0%BD%D1%8B%D0%B5%20%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%D1%8B%20%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D0%B8/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4%20%D0%BE%D0%BF%D0%BE%D1%80%D0%BD%D1%8B%D1%85%20%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BE%D0%B2/img/Screenshot%202020-11-17%20at%2015.14.03.png" width="40%">


Предположим, что выборка линейно разделима и существуют значения параметров w, w0, при которых функционал числа ошибок
<br/>
<img src="https://github.com/davilexx/ml1/blob/master/%D0%9B%D0%B8%D0%BD%D0%B5%D0%B8%CC%86%D0%BD%D1%8B%D0%B5%20%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%D1%8B%20%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D0%B8/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4%20%D0%BE%D0%BF%D0%BE%D1%80%D0%BD%D1%8B%D1%85%20%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BE%D0%B2/img/Screenshot%202020-11-17%20at%2015.14.09.png" width="40%">
<br/>
принимает нулевое значение. Но тогда разделяющая гиперплоскость не единственна. Можно выбрать другие её положения, реализующие такое же разбиение выборки
на два класса. Идея метода заключается в том, чтобы разумным образом распорядиться этой свободой выбора.


<b>Оптимальная разделяющая гиперплоскость.</b> Потребуем, чтобы разделяющая гиперплоскость максимально далеко отстояла от ближайших к ней точек обоих классов. Первоначально данный принцип классификации возник из эвристических соображений: вполне естественно полагать, что максимизация зазора (margin) между классами должна способствовать более надёжной классификации.


<b>Нормировка.</b> Заметим, что параметры линейного порогового классификатора определены с точностью до нормировки: алгоритм a(x) не изменится, если w и w0 одновременно умножить на одну и ту же положительную константу. Удобно выбрать эту
константу таким образом, чтобы выполнялось условие
<br/>
<img src="https://github.com/davilexx/ml1/blob/master/%D0%9B%D0%B8%D0%BD%D0%B5%D0%B8%CC%86%D0%BD%D1%8B%D0%B5%20%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%D1%8B%20%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D0%B8/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4%20%D0%BE%D0%BF%D0%BE%D1%80%D0%BD%D1%8B%D1%85%20%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BE%D0%B2/img/Screenshot%202020-11-17%20at%2015.14.17.png" width="30%">
<br/>


Множество точек описывает полосу, разделяющую классы (рис. 1). Ни один из объектов обучающей выборки не попадает внутрь этой полосы. Границами полосы служат две параллельные гиперплоскости с вектором нормали w. Разделяющая гиперплоскость проходит ровно по середине между ними. Объекты, ближайшие к разделяющей гиперплоскости, лежат на границах полосы, и именно на них достигается минимум. В каждом из классов имеется хотя бы один такой объект, в противном случае разделяющую полосу можно было бы ещё немного расширить и нарушался бы принцип максимального зазора.
<br/>
<img src="https://github.com/davilexx/ml1/blob/master/%D0%9B%D0%B8%D0%BD%D0%B5%D0%B8%CC%86%D0%BD%D1%8B%D0%B5%20%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%D1%8B%20%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D0%B8/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4%20%D0%BE%D0%BF%D0%BE%D1%80%D0%BD%D1%8B%D1%85%20%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BE%D0%B2/img/Screenshot%202020-11-17%20at%2015.14.35.png" width="30%">
<br/>
<p style="color:gray">Рисунок 1. Линейно разделимая выборка. Обучающие объекты x− и x+ находятся на границе разделяющей полосы. Вектор нормали w к разделяющей гиперплоскости определяет ширину полосы.</p>
