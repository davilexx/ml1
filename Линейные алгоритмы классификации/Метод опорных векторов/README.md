
# Метод опорных векторов (SVM)

<b>Метод SVM (support vector machine)</b> обладает несколькими свойствами:
<br/>
• обучение SVM сводится к задаче квадратичного программирования, имеющей единственное решение, которое вычисляется достаточно эффективно даже на выборках
в сотни тысяч объектов;
<br/>
• решение обладает свойством разреженности: положение оптимальной разделяющей гиперплоскости зависит лишь от небольшой
доли обучающих объектов, которые и называются опорными векторами, остальные объекты фактически не задействуются;
<br/>
• с помощью введения функции ядра метод обобщается на случай нелинейных разделяющих поверхностей.
<br/>

<h2>Линейно разделимая выборка</h2>

Рассмотрим задачу классификации на два непересекающихся класса, в которой объекты описываются n-мерными вещественными векторами:
<br/>
<img src="https://github.com/davilexx/ml1/blob/master/%D0%9B%D0%B8%D0%BD%D0%B5%D0%B8%CC%86%D0%BD%D1%8B%D0%B5%20%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%D1%8B%20%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D0%B8/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4%20%D0%BE%D0%BF%D0%BE%D1%80%D0%BD%D1%8B%D1%85%20%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BE%D0%B2/img/Screenshot%202020-11-17%20at%2015.13.47.png" width="30%">

Будем строить линейный пороговый классификатор:
<br/>
<img src="https://github.com/davilexx/ml1/blob/master/%D0%9B%D0%B8%D0%BD%D0%B5%D0%B8%CC%86%D0%BD%D1%8B%D0%B5%20%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%D1%8B%20%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D0%B8/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4%20%D0%BE%D0%BF%D0%BE%D1%80%D0%BD%D1%8B%D1%85%20%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BE%D0%B2/img/Screenshot%202020-11-17%20at%2015.14.03.png" width="50%">


Предположим, что выборка линейно разделима и существуют значения параметров w, w0, при которых функционал числа ошибок
<br/>
<img src="https://github.com/davilexx/ml1/blob/master/%D0%9B%D0%B8%D0%BD%D0%B5%D0%B8%CC%86%D0%BD%D1%8B%D0%B5%20%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%D1%8B%20%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D0%B8/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4%20%D0%BE%D0%BF%D0%BE%D1%80%D0%BD%D1%8B%D1%85%20%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BE%D0%B2/img/Screenshot%202020-11-17%20at%2015.14.09.png" width="40%">
<br/>
принимает нулевое значение. Но тогда разделяющая гиперплоскость не единственна. Можно выбрать другие её положения, реализующие такое же разбиение выборки
на два класса. Идея метода заключается в том, чтобы разумным образом распорядиться этой свободой выбора.

<b>Оптимальная разделяющая гиперплоскость.</b> Потребуем, чтобы разделяющая гиперплоскость максимально далеко отстояла от ближайших к ней точек обоих классов. Первоначально данный принцип классификации возник из эвристических соображений: вполне естественно полагать, что максимизация зазора (margin) между классами должна способствовать более надёжной классификации.

<b>Нормировка.</b> Заметим, что параметры линейного порогового классификатора определены с точностью до нормировки: алгоритм a(x) не изменится, если w и w0 одновременно умножить на одну и ту же положительную константу. Удобно выбрать эту константу таким образом, чтобы выполнялось условие
<br/>
<img src="https://github.com/davilexx/ml1/blob/master/%D0%9B%D0%B8%D0%BD%D0%B5%D0%B8%CC%86%D0%BD%D1%8B%D0%B5%20%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%D1%8B%20%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D0%B8/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4%20%D0%BE%D0%BF%D0%BE%D1%80%D0%BD%D1%8B%D1%85%20%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BE%D0%B2/img/Screenshot%202020-11-17%20at%2015.14.17.png" width="30%">
<br/>


Множество точек описывает полосу, разделяющую классы (рис. 1). Ни один из объектов обучающей выборки не попадает внутрь этой полосы. Границами полосы служат две параллельные гиперплоскости с вектором нормали w. Разделяющая гиперплоскость проходит ровно по середине между ними. Объекты, ближайшие к разделяющей гиперплоскости, лежат на границах полосы, и именно на них достигается минимум. В каждом из классов имеется хотя бы один такой объект, в противном случае разделяющую полосу можно было бы ещё немного расширить и нарушался бы принцип максимального зазора.
<br/>
<img src="https://github.com/davilexx/ml1/blob/master/%D0%9B%D0%B8%D0%BD%D0%B5%D0%B8%CC%86%D0%BD%D1%8B%D0%B5%20%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%D1%8B%20%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D0%B8/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4%20%D0%BE%D0%BF%D0%BE%D1%80%D0%BD%D1%8B%D1%85%20%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BE%D0%B2/img/Screenshot%202020-11-17%20at%2015.14.35.png" width="50%">
<br/>
<b>Рисунок 1.</b> Линейно разделимая выборка. Обучающие объекты x− и x+ находятся на границе разделяющей полосы. Вектор нормали w к разделяющей гиперплоскости определяет ширину полосы.

Ширина разделяющей полосы. Чтобы разделяющая гиперплоскость как можно дальше отстояла от точек выборки, ширина полосы должна быть максимальной. Пусть x− и x+ — два обучающих объекта классов −1 и +1 соответственно, лежащие на границе полосы. Тогда ширина полосы есть
<br/>
<img src="https://github.com/davilexx/ml1/blob/master/%D0%9B%D0%B8%D0%BD%D0%B5%D0%B8%CC%86%D0%BD%D1%8B%D0%B5%20%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%D1%8B%20%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D0%B8/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4%20%D0%BE%D0%BF%D0%BE%D1%80%D0%BD%D1%8B%D1%85%20%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BE%D0%B2/img/Screenshot%202020-11-17%20at%2015.14.40.png" width="70%">
<br/>

Ширина полосы максимальна, когда норма вектора w минимальна.
<br/>
Итак, в случае линейно разделимой выборки получаем задачу квадратичного программирования: требуется найти значения параметров w и w0, при которых
выполняются ℓ ограничений-неравенств и норма вектора w минимальна:
<br/>
<img src="https://github.com/davilexx/ml1/blob/master/%D0%9B%D0%B8%D0%BD%D0%B5%D0%B8%CC%86%D0%BD%D1%8B%D0%B5%20%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%D1%8B%20%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D0%B8/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4%20%D0%BE%D0%BF%D0%BE%D1%80%D0%BD%D1%8B%D1%85%20%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BE%D0%B2/img/Screenshot%202020-11-17%20at%2015.14.45.png" width="40%">
<br/>
На практике линейно разделимые классы встречаются довольно редко. Поэтому постановку задачи необходимо модифицировать так, чтобы система ограничений была совместна в любой ситуации.

<h2>Линейно неразделимая выборка</h2>
Чтобы обобщить постановку задачи на случай линейно неразделимой выборки, позволим алгоритму допускать ошибки на обучающих объектах, но при этом постараемся, чтобы ошибок было поменьше. Введём дополнительные переменные, характеризующие величину ошибки на объектах. Ослабим в ограничения-неравенства и одновременно введём в минимизируемый функционал штраф за суммарную ошибку:
<br/>
<img src="https://github.com/davilexx/ml1/blob/master/%D0%9B%D0%B8%D0%BD%D0%B5%D0%B8%CC%86%D0%BD%D1%8B%D0%B5%20%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%D1%8B%20%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D0%B8/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4%20%D0%BE%D0%BF%D0%BE%D1%80%D0%BD%D1%8B%D1%85%20%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BE%D0%B2/img/Screenshot%202020-11-17%20at%2015.14.49.png" width="40%">
<br/>

Положительная константа C является управляющим параметром метода и позволяет находить компромисс между максимизацией ширины разделяющей полосы
и минимизацией суммарной ошибки.
<br/>
<img src="https://github.com/davilexx/ml1/blob/master/%D0%9B%D0%B8%D0%BD%D0%B5%D0%B8%CC%86%D0%BD%D1%8B%D0%B5%20%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%D1%8B%20%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D0%B8/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4%20%D0%BE%D0%BF%D0%BE%D1%80%D0%BD%D1%8B%D1%85%20%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BE%D0%B2/img/Screenshot%202020-11-17%20at%2015.14.54.png" width="50%">
<br/>
<b>Рисунок 2.</b> Кусочно-линейная аппроксимация пороговой функции потерь.

Регуляризация эмпирического риска. В задачах с двумя классами Y = {−1, +1} отступом (margin) объекта xi от границы классов называется величина
<br/>
<img src="https://github.com/davilexx/ml1/blob/master/%D0%9B%D0%B8%D0%BD%D0%B5%D0%B8%CC%86%D0%BD%D1%8B%D0%B5%20%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%D1%8B%20%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D0%B8/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4%20%D0%BE%D0%BF%D0%BE%D1%80%D0%BD%D1%8B%D1%85%20%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BE%D0%B2/img/Screenshot%202020-11-17%20at%2015.55.12.png" width="40%">
<br/>


Алгоритм допускает ошибку на объекте xi тогда и только тогда, когда отступ Mi отрицателен. Если Mi ∈ (−1, +1), то объект xi попадает внутрь разделяющей полосы. Если Mi > 1, то объект xi классифицируется правильно, и находится на некотором удалении от разделяющей полосы.
<br/>
Ошибка ξi выражается через отступ Mi. Действительно, из ограничений-неравенств следует, что ξi > 0 и ξi > 1−Mi. В силу требования минимизации
суммы одно из этих неравенств обязательно должно обратиться в равенство. Следовательно, ξi = (1 − Mi)+. Таким образом, задача оказывается эквивалентной безусловной минимизации функционала Q, не зависящего от переменных ξi.
<br/>
<img src="https://github.com/davilexx/ml1/blob/master/%D0%9B%D0%B8%D0%BD%D0%B5%D0%B8%CC%86%D0%BD%D1%8B%D0%B5%20%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%D1%8B%20%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D0%B8/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4%20%D0%BE%D0%BF%D0%BE%D1%80%D0%BD%D1%8B%D1%85%20%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BE%D0%B2/img/Screenshot%202020-11-17%20at%2015.14.59.png" width="60%">
<br/>


В силу неравенства, проиллюстрированного на рисунке 14, функционал можно рассматривать как верхнюю оценку эмпирического риска (числа ошибочных классификаций объектов обучающей выборки), к которому добавлен регуляризатор,
умноженный на параметр регуляризации.
<br/>
Замена пороговой функции потерь кусочно-линейной верхней оценкой L (M) = (1 − M)+ делает функцию потерь чувствительной к величине ошибки. Функция потерь L(M) штрафует объекты за приближение к границе классов.
<br/>
Введение регуляризатора повышает устойчивость решения w. В случаях, когда минимум эмпирического риска достигается на множестве векторов w, регуляризация выбирает из них вектор с минимальной нормой. Тем самым устраняется проблема мультиколлинеарности, повышается устойчивость алгоритма, улучшается его обобщающая способность. Таким образом, принцип оптимальной разделяющей гиперплоскости или максимизации ширины разделяющей полосы тесно связан с регуляризацией некорректно поставленных задач.
<br/>
Задача соответствует принципу максимума совместного правдоподобия, если принять модель плотности
<br/>
<img src="https://github.com/davilexx/ml1/blob/master/%D0%9B%D0%B8%D0%BD%D0%B5%D0%B8%CC%86%D0%BD%D1%8B%D0%B5%20%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%D1%8B%20%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D0%B8/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4%20%D0%BE%D0%BF%D0%BE%D1%80%D0%BD%D1%8B%D1%85%20%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BE%D0%B2/img/Screenshot%202020-11-17%20at%2015.15.05.png" width="50%">
<br/>
гауссовскую модель априорного распределения вектора параметров w
<br/>
Задача соответствует принципу максимума совместного правдоподобия, если принять модель плотности
<br/>
<img src="https://github.com/davilexx/ml1/blob/master/%D0%9B%D0%B8%D0%BD%D0%B5%D0%B8%CC%86%D0%BD%D1%8B%D0%B5%20%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%D1%8B%20%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D0%B8/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4%20%D0%BE%D0%BF%D0%BE%D1%80%D0%BD%D1%8B%D1%85%20%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BE%D0%B2/img/Screenshot%202020-11-17%20at%2015.15.09.png" width="40%">
<br/>
